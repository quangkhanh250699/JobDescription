{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"Simple App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think of it for a moment – 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.',\n",
       " '',\n",
       " 'Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.',\n",
       " '',\n",
       " 'On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sc.textFile('../data/blogtexts')\n",
    "\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_split(lines): \n",
    "    lines = lines.lower() \n",
    "    lines = lines.split() \n",
    "    return lines\n",
    "\n",
    "df1 = df.flatMap(lower_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', 'of', 'it', 'moment', '–', '1', 'qunitillion', '=', '1', 'million']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['is','am','are','the','for','a']\n",
    "df2 = df1.filter(lambda x: x not in stopwords) \n",
    "df2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thi', <pyspark.resultiterable.ResultIterable at 0x7f06b1aa6940>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2.groupBy(lambda x: x[:3]) \n",
    "[(k, v) for k, v in df3.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['think', 'of', 'it']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.map(lambda x: re.sub(r'[.,!:]', '', x))\n",
    "df2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.map(lambda x: (x, 1)) \n",
    "df3_mapped = df3.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('think', [1, 1])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, list(j)) for i, j in df3_mapped.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 'it'),\n",
       " (1, 'moment'),\n",
       " (1, 'cds'),\n",
       " (1, 'professional'),\n",
       " (6, 'created'),\n",
       " (10, 'us'),\n",
       " (15, 'now'),\n",
       " (1, 'become'),\n",
       " (1, 'differentiation'),\n",
       " (3, 'better'),\n",
       " (22, 'if'),\n",
       " (2, 'did'),\n",
       " (5, 'your'),\n",
       " (1, 'complete'),\n",
       " (1, 'knowledge'),\n",
       " (2, 'introduction'),\n",
       " (1, 'contents'),\n",
       " (9, 'computing'),\n",
       " (6, 'dataset'),\n",
       " (1, 'emanating'),\n",
       " (1, 'deal'),\n",
       " (3, 'efficiently'),\n",
       " (1, 'sense'),\n",
       " (1, 'network'),\n",
       " (4, 'known'),\n",
       " (1, 'aggregate'),\n",
       " (5, 'get'),\n",
       " (6, 'just'),\n",
       " (3, 'certain'),\n",
       " (6, 'same'),\n",
       " (1, 'useful'),\n",
       " (1, 'books'),\n",
       " (1, 'finish'),\n",
       " (1, 'less'),\n",
       " (1, 'hour'),\n",
       " (1, 'do?'),\n",
       " (1, 'report'),\n",
       " (1, 'minutes'),\n",
       " (8, 'computation'),\n",
       " (1, 'amounts'),\n",
       " (4, 'parallel'),\n",
       " (1, 'california'),\n",
       " (13, 'some'),\n",
       " (2, 'include'),\n",
       " (1, 'https//sparkapacheorg/docs/111/img/cluster-overviewpng'),\n",
       " (11, 'program'),\n",
       " (4, 'application'),\n",
       " (4, 'types'),\n",
       " (5, 'so'),\n",
       " (1, 'independently'),\n",
       " (3, 'talk'),\n",
       " (1, 'figure'),\n",
       " (2, 'down'),\n",
       " (1, 'gained'),\n",
       " (1, 'convert'),\n",
       " (2, 'happening'),\n",
       " (5, 'case'),\n",
       " (1, 'saved'),\n",
       " (1, 'solutions'),\n",
       " (4, 'transformations'),\n",
       " (1, 'comparison'),\n",
       " (5, 'here'),\n",
       " (1, 'way'),\n",
       " (1, '(spark'),\n",
       " (5, 'path'),\n",
       " (1, '1404'),\n",
       " (1, 'windows'),\n",
       " (1, 'ppawebupd8team/java'),\n",
       " (1, 'mechanism'),\n",
       " (1, 'sbt'),\n",
       " (1, 'lower'),\n",
       " (1, 'logger'),\n",
       " (1, '6'),\n",
       " (1, 'them)'),\n",
       " (1, 'ln'),\n",
       " (2, 'lines'),\n",
       " (1, 'spark_home=/opt/spark'),\n",
       " (1, '~/ipython/profile_default/startup/load_spark_environment_variablespy'),\n",
       " (1, \"'/opt/spark/python'\"),\n",
       " (1, \"'/opt/spark/python')\"),\n",
       " (1, '8'),\n",
       " (1, 'depth'),\n",
       " (1, '(transformations'),\n",
       " (1, 'actions)'),\n",
       " (1, 'tolerant'),\n",
       " (1, 'mind'),\n",
       " (5, 'shared'),\n",
       " (2, 'copy'),\n",
       " (3, 'broadcast'),\n",
       " (1, 'aggregating'),\n",
       " (3, 'list'),\n",
       " (1, 'input'),\n",
       " (2, 'providing'),\n",
       " (1, '(like'),\n",
       " (2, 'partitions'),\n",
       " (1, \"'ankit\"),\n",
       " (1, '(name'),\n",
       " (2, 'lambda'),\n",
       " (3, 'rdd1'),\n",
       " (1, 'fine'),\n",
       " (1, 'folder'),\n",
       " (2, 'impute'),\n",
       " (1, \"trainnadrop()count()testnadrop('any')count()\"),\n",
       " (7, 'transform'),\n",
       " (4, 'numerical'),\n",
       " (1, 'summary'),\n",
       " (1, 'shows'),\n",
       " (1, 'traindescribe()show()'),\n",
       " (2, '46'),\n",
       " (7, 'test1'),\n",
       " (1, 'gender\"featurescol=\"features\"labelcol=\"label\")'),\n",
       " (1, 'fitting'),\n",
       " (1, 'formulafit(train1)'),\n",
       " (1, 'algorithm'),\n",
       " (1, 'squae'),\n",
       " (1, 'numpy'),\n",
       " (2, 'submission'),\n",
       " (1, 'predictions1selectexpr(\"user_id'),\n",
       " (1, \"purchase')\"),\n",
       " (1, 'bad'),\n",
       " (1, 'weeks'),\n",
       " (1, 'continue'),\n",
       " (1, 'digest'),\n",
       " (1, 'guide'),\n",
       " (1, 'questions')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_mapped.mapValues(lambda x: sum(x)).map(lambda x: (x[1], x[0])).sample(fraction=0.1, withReplacement=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4768, 1238)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2.collect()), len(df2.distinct().collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
